{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error **`'tor' is not recognized as an internal or external command`** means that the `tor` command is not in your system's PATH, or Tor is not properly installed. Follow these steps to resolve the issue:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Download and Install Tor**\n",
    "\n",
    "### **On Windows**:\n",
    "1. Go to the official Tor website: [https://www.torproject.org/download/](https://www.torproject.org/download/).\n",
    "2. Download the **Tor Browser** and install it. By default, Tor is bundled with the Tor Browser.\n",
    "\n",
    "### **Find the `tor.exe` Executable**:\n",
    "1. After installing Tor Browser:\n",
    "   - Navigate to the installation folder. Typically, it is:\n",
    "     ```\n",
    "     C:\\Users\\<YourUsername>\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Tor\n",
    "     ```\n",
    "2. In this folder, you will find **`tor.exe`**.\n",
    "\n",
    "### **Run Tor with the Full Path**:\n",
    "You can run `tor.exe` directly by specifying the full path in Command Prompt.\n",
    "\n",
    "Example:\n",
    "```cmd\n",
    "\"C:\\Users\\<YourUsername>\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Tor\\tor.exe\" --hash-password your_password\n",
    "```\n",
    "Replace `<YourUsername>` with your Windows username and `your_password` with the desired password.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Add Tor to Your System PATH (Optional)**\n",
    "\n",
    "To run the `tor` command from any directory without specifying the full path:\n",
    "\n",
    "1. Copy the folder path where `tor.exe` is located, for example:\n",
    "   ```\n",
    "   C:\\Users\\<YourUsername>\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Tor\n",
    "   ```\n",
    "\n",
    "2. Add the path to the **system PATH**:\n",
    "   - Press `Win + R`, type `sysdm.cpl`, and press **Enter**.\n",
    "   - Go to the **Advanced** tab and click on **Environment Variables**.\n",
    "   - In the **System variables** section, select **Path** and click **Edit**.\n",
    "   - Click **New** and paste the folder path.\n",
    "   - Click **OK** to save.\n",
    "\n",
    "3. Restart Command Prompt.\n",
    "\n",
    "Now you can run `tor --hash-password your_password` from any location.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Test the Tor Command**\n",
    "After adding Tor to your PATH, verify the installation by running:\n",
    "```cmd\n",
    "tor --version\n",
    "```\n",
    "This should display the installed Tor version.\n",
    "\n",
    "---\n",
    "\n",
    "### **Troubleshooting Tips**:\n",
    "- If the Tor executable still doesn't work, ensure there are no typos in the file path.\n",
    "- Confirm that Tor Browser is fully installed and not missing any components.\n",
    "\n",
    "Now you should be able to generate a hashed password for Tor using the command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the title and prices of the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve page. Status code: 503\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up Tor connection\n",
    "proxies = {\n",
    "    'http': 'socks5h://127.0.0.1:9050',  # Tor proxy\n",
    "    'https': 'socks5h://127.0.0.1:9050'  # Tor proxy\n",
    "}\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ebay.com/sch/i.html?_nkw=iphone&_sop=12&_kwlnd=1\"\n",
    "\n",
    "# Request headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to fetch page content\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Parse product data from HTML\n",
    "def parse_product_data(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    products = []\n",
    "    for item in soup.select('.s-item'):\n",
    "        title_tag = item.select_one('.s-item__title')\n",
    "        price_tag = item.select_one('.s-item__price')\n",
    "\n",
    "        title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "        price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "\n",
    "        products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": price\n",
    "        })\n",
    "    return products\n",
    "\n",
    "# Write data to CSV\n",
    "def write_to_csv(data, filename='products.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Title\", \"Price\"])\n",
    "        writer.writeheader() \n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        products = parse_product_data(html_content)\n",
    "        write_to_csv(products)\n",
    "        print(f\"Data saved to products.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code we save the Images Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve page. Status code: 503\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up Tor connection\n",
    "proxies = {\n",
    "    'http': 'socks5h://127.0.0.1:9050',  # Tor proxy\n",
    "    'https': 'socks5h://127.0.0.1:9050'  # Tor proxy\n",
    "}\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ebay.com/sch/i.html?_nkw=iphone&_sop=12&_kwlnd=1\"\n",
    "\n",
    "# Request headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to fetch page content\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Parse product data from HTML\n",
    "def parse_product_data(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    products = []\n",
    "    for item in soup.select('.s-item'):\n",
    "        title_tag = item.select_one('.s-item__title')\n",
    "        price_tag = item.select_one('.s-item__price')\n",
    "        image_tag = item.select_one('.s-item__image-img')\n",
    "\n",
    "        title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "        price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "        image_url = image_tag['src'] if image_tag and 'src' in image_tag.attrs else \"N/A\"\n",
    "\n",
    "        products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Image URL\": image_url\n",
    "        })\n",
    "    return products\n",
    "\n",
    "# Write data to CSV\n",
    "def write_to_csv(data, filename='products1.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Title\", \"Price\", \"Image URL\"])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        products = parse_product_data(html_content)\n",
    "        write_to_csv(products)\n",
    "        print(f\"Data saved to products.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save Url into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up Tor connection\n",
    "proxies = {\n",
    "    'http': 'socks5h://127.0.0.1:9050',  # Tor proxy\n",
    "    'https': 'socks5h://127.0.0.1:9050'  # Tor proxy\n",
    "}\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ebay.com/sch/i.html?_nkw=iphone&_sop=12&_kwlnd=1\"\n",
    "\n",
    "# Request headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to fetch page content\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Parse product data from HTML\n",
    "def parse_product_data(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    image_urls = []\n",
    "    for img_tag in soup.select('img'):\n",
    "        image_url = img_tag.get('src')\n",
    "        if image_url:\n",
    "            image_urls.append(image_url)\n",
    "    return image_urls\n",
    "\n",
    "# Write data to CSV\n",
    "def write_to_csv(data, filename='images.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Image URL'])\n",
    "        for url in data:\n",
    "            writer.writerow([url])\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        image_urls = parse_product_data(html_content)\n",
    "        write_to_csv(image_urls)\n",
    "        print(f\"Image URLs saved to images.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the images into PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Set up Tor connection\n",
    "proxies = {\n",
    "    'http': 'socks5h://127.0.0.1:9050',  # Tor proxy\n",
    "    'https': 'socks5h://127.0.0.1:9050'  # Tor proxy\n",
    "}\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ebay.com/sch/i.html?_nkw=iphone&_sop=12&_kwlnd=1\"\n",
    "\n",
    "# Request headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to fetch page content\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Parse product data from HTML\n",
    "def parse_image_data(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    image_data = []\n",
    "    for img_tag in soup.select('img'):\n",
    "        image_url = img_tag.get('src')\n",
    "        if image_url:\n",
    "            image_data.append(image_url)\n",
    "    return image_data\n",
    "\n",
    "# Download and save images and write data into CSV\n",
    "def download_images_and_write_csv(image_urls, folder='images_with_data'):\n",
    "    os.makedirs(folder, exist_ok=True)  # Create directory if not exists\n",
    "    image_data_list = []\n",
    "    for idx, image_url in enumerate(image_urls):\n",
    "        try:\n",
    "            image_data = requests.get(image_url, headers=headers, proxies=proxies)\n",
    "            if image_data.status_code == 200:\n",
    "                image_filename = os.path.join(folder, f'image_{idx + 1}.jpg')\n",
    "                with open(image_filename, 'wb') as f:\n",
    "                    f.write(image_data.content)\n",
    "                image_data_list.append(image_filename)  # Store the image path for CSV\n",
    "                print(f\"Downloaded: {image_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {image_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {image_url}: {e}\")\n",
    "    return image_data_list\n",
    "\n",
    "# Write image URLs and paths to CSV\n",
    "def write_to_csv(image_data_list, filename='image_data.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Image Path', 'Image URL'])\n",
    "        for image_path, image_url in zip(image_data_list, image_urls):\n",
    "            writer.writerow([image_path, image_url])\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        image_urls = parse_image_data(html_content)\n",
    "        image_paths = download_images_and_write_csv(image_urls)\n",
    "        write_to_csv(image_paths, 'image_data.csv')\n",
    "        print(f\"Image URLs and images saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
